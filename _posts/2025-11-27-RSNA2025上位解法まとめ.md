---
layout: article
title: RSNA2025上位解法まとめ
author: K. Hikaru
tags:
  - Kaggle
date: '2025-11-30'
published: false
---
# はじめに

[RSNA Intracranial Aneurysm Detection](https://www.kaggle.com/competitions/rsna-intracranial-aneurysm-detection/overview)というKaggleのコンペに参加しました。自分の復習も兼ねて上位（1st-9th）の解法を読んだので、それをまとめます。特に解法にある図を理解したいと思ったので、図をメインで見ています。

> 筆者がポンコツなので、記事にミスがある、重要なところを見逃している可能性があります。記事の最後に各解法のリンクをつけていますので、参考にしてください。（間違いに気づいたらこっそり教えてください）

### その他の工夫など


# コンペ概要

## 説明

- 頭蓋内脳動脈瘤は世界人口の3％に影響を与え、毎年50万人以上が亡くなっており、その約半数は50歳未満です。

- 今回のコンペでは様々な施設から得られたデータを用いて、実際の臨床的なバイアス（施設ごとの機械の差や、画像のコントラストの違いなど）を考慮した汎用的なモデルの構築が課題とされました。

- 評価は、動脈瘤が存在するか(AP)と、脳動脈瘤の位置に対応した13個のラベル合わせて14個のラベルについてのAUCスコアになります。


## データ

学習にはCTA、MRA、T1MRI、T2MRIなどの様々なモダリティで撮影された約4000人分の画像が提供されました。

- seriesは各患者一人ごとに与えられ、このseriesの中に頭部全体の画像が並べられています。データ形式はdicomでした。

- `train.csv`には動脈瘤の有無と13個の位置についての情報のラベル、その他年齢や性別といった情報も載せられていました。

- `train_localizer.csv`で動脈瘤の中心座標が与えられていました。

- 一部のseries(200個くらい)には、血管セグメンテーションも提供されていました。

# 上位解法まとめ
- どのチームも、マルチステージを採用していました。
- 
## 1位解法
### 1st stage : nnU-Netを用いたROI抽出
![image.png]({{ site.baseurl }}/assets/images/1764402539603-image.png)

 #### Preprocessed volume 
-  dicomデータをNIfTIに変換、そのあと座標系をRAS揃えて読み込み、nnUnetの前処理を加えています。

#### Coare Scan
-  脳スキャンデータを等方1mmにリサンプリングして粗いnnU-Net(Model1)に放りこみます。このモデルは出力として各ボクセルに対して、13+1クラスではなく4クラスのラベル（0:背景 + 3つの血管グループ）を返します。
- 後処理として、①ラベルを血管あり/血管なしの2値化をして,、②血管ありのボクセルの座標をすべて読み取り、③`DBSCAN`を用いてクラスタリングを施し、④一番大きいクラスタの中心座標を求め、⑤その座標を中心とした140mmのROIをクロップしてボクセルに変換しています。

#### Fine Inference
- 先ほど頑張って取得したROIを(0.80, 0.45, 0.44)にリサンプリングします(この数値は、nnU-Netの学習結果から持ってきたそうです) 。このROIを設定の違うnnU-NetであるModel2とModel3に入力します。

- Model2は損失関数がDice + CE + SkeltonRecall(weight=1)でModel3はTversky + CE + SkeltonRecall(weight=3)です。前者ではバランス重視、後者では見逃しをなくすのが狙いといった感じなのだと思います。

- `ROI refine`では、Model2のセグメンテーションの結果から最小のbboxを作成し、それにmarginを加えてROIを作成します。それを分類モデルの入力サイズに合わせて補完して、Final ROIが作成されます。この時、同時に13クラスの血管マスクとをModel2とModel3それぞれから出力しています。

### 2nd stage : 13クラス + 動脈瘤有無 + 補助タスク
#### nnU-Net Backbone
![image.png]({{ site.baseurl }}/assets/images/1764412257679-image.png)

- この図は、先ほどの出力をどのようにして最終的な予測に変えているかを表しています。

- 左上のROI Volumeが先ほどのFinal ROIに対応しています。Vessel maskはModel2/3の出力を合わせたものです。

- nnU-Net Backboneでは、`1 × 128 × 256 × 256`のROIからエンコーダ層の最後で`320 × 8 × 8 × 8`の出力を、デコーダ層の最後で`64 × 64 × 128 × 128`の出力を与えています。前者は最終的にROIの特徴を集約した64次元のベクトルになり、後者は補助タスクやUnion(動脈瘤が存在するかどうか)、13クラスの推定に使われています。

- `Aneurysm Detection Head`では、nnU-Netのデコーダ層の特徴に対してConvヘッドをつけ、補助タスクとして動脈瘤の存在する球を予測しロスを計算しています。これの教師データはおそらく`train_localizer.csv`を使用していると思われます。

#### Vessel Region-Masked Pooling
![image.png]({{ site.baseurl }}/assets/images/1764414914932-image.png)

-  この図は、先ほどの図にもあった`Vessel Region-Masked Pooling`を解説しています。Vessel Maskをダウンサンプリングしてデコーダと形をそろえ、Decoderの特徴量と掛けます。Maskは0-1の値をとるので、ここで掛けることで、例えばMaskが0のところの特徴は0に、Maskが1のところの特徴は1になり、どこにモデルが注目したらいいかがわかりやすくなります。掛けた特徴量を各チャネルでマスク内の平均をとり出力とします。

- 一個前の図ではUnionとありましたが、これは13クラスを動脈瘤が存在するかどうかの1クラスにまとめているだけで、あとは同じです。

#### Location-Aware Transformer
![image.png]({{ site.baseurl }}/assets/images/1764416118197-image.png)

- この図は、2つ前の図にあった`Location-Aware Transformer`の入力について解説しているものです。
- 緑の特徴量は、先ほどの血管マスクで取得した特徴量です。それに、nnU-Netのエンコーダ層からとってきた特徴量を13か所分コピーします。それを`Linear Projection`ブロックで、Transformer用の埋め込みベクトルにしています。

### その他の工夫など
- フェイルセーフ機構を用いて、セグメンテーションあるいはROI抽出で異常が発生した場合はoofの確率を返すようにしていた。
- 推論時にはTTAを使用していた。
- データには強いAugmentationをかけていた。
- TensorRTで高速化していた。

## 2位解法

### データの前処理

- 今回のデータセットには、①モダリティがバラバラ、②スライス(画像の間隔)がバラバラ、③1ファイルに複数のフレームが入っているなどの理由で画一的な前処理をするのが困難でした。

- dicomデータをpydicomを用いて、SliceThicknessといったメタデータがないときでもスライス間隔を予想する処理などを用意していました。

- また、T2画像は画像の向きがほかの撮影系列とは異なることがあるため、T2固有の方向分類器を作成しました。

### 1st stage

![image.png]({{ site.baseurl }}/assets/images/1764429623005-image.png)

- axial, sagittal, coronalの三方向から、それぞれ3枚ずつ計9枚スライスを取得します。

- 2D画像を2D nn-Unetに入力し、背景と血管で2値マスクを作成します。

- これにより、axial, sagittal, coronal三方向のマスクが作成され、これをもとに3次元のROIをクロップできます。

### 2nd stage

![image.png]({{ site.baseurl }}/assets/images/1764432101942-image.png)

- この図は、先ほど作成したROIを入力としたnnU-Net + Cross Attention分類機構を示しています。

- nnU-Net ResEncからの出力`[B, 320, 7, 7,7]`を`[7*7*7, B, 320]`に変換して、それをCross attenntion poolingに入力します。ここで、動脈瘤の有無、13クラス分類、モダリティ分類でqueryとHeadを分けています。

- また、血管セグメンテーション(左側)と動脈瘤セグメンテーション(右側)を同時に学習しており、これにより学習が安定したとありました。

### セグメンテーションについて
- 今回のデータには血管セグメンテーションが一部しかないのに、どのように学習しているのか疑問に思ったのですこし深掘りしていきます。

-  セグメンテーションタスクはstage1とstage2の両方でありますが、これにはTopCowという外部データセットから学習したモデルを使っています。

#### 1st stage 

- 外部データセットにはCTA/MRAの血管セグメンテーションがあるので、これを単純に学習した2Dのセグメンテーションタスクを学習したモデルを使ってROIを作成しています。

#### 2nd stage 

- 血管セグメンテーションについて、TopCoW / TopBrain データセットには、ウィリス動脈輪などのフル3D血管セグメ + 解剖学クラスラベルがあるので、。
これを使ってCoW セグメントモデルを先に作っています。これをRSNAのデータに適応してデータを作成し、手動で微調整することで疑似マスクを作成しています。

- 動脈瘤セグメンテーションについて、RSNAデータセットに動脈瘤の中心座標のデータがあるので、それを2~5mmのランダムな立方体を作成してボクセルに変換します。このボックスマスクでU-Netを学習させます。これだとまだ粗いので、モデルの出力からさらに洗練されたマスクを作成します。具体的には、①モデルからの出力を閾値を設けてバイナリにし、②3D connected componentで塊に分けて、③中心点から一番近い塊だけを残し、④さらに、血管セグメンテーションを利用して血管から離れすぎているところを削り、⑤新しい教師画像として利用する、ということをしているようです。

### そのほかの工夫

- TTAではラベルと画像を左右反転したものも加えていました。

- 前処理の実装を徹底して行い、予測値のフォールバックを行わず、すべての画像を読み込めるようにしました。

## 3位解法

### 1st stage
 
![image.png]({{ site.baseurl }}/assets/images/1764480696186-image.png)

### 1st stage

- 図の上のほうです。

- まず、もとの3D volumeのセグメンテーションマスクから、Sagittal とCoronal 方向で2D のMIP(最大値投影)画像を生成します。このセグメンテーションマスクがどのように作成されているかはよくわかりませんでした(´;ω;｀)。(もしかしたら、血管セグメンテーションが用意してある200例だけを教師にしているのかもしれません。)

- 先ほど作成してMIP画像に対してYOLO v8を用いてBBoxの検出を行いそれをもとにROIを作成しました。
 
### 2nd stage

- 図の下のほうです。

- 先ほど作成した3D ROIを3D ResNet-18に入力します。3Dモデルなので、出力も3Dです。ここの出力はデフォルトで`4 * 4 * 4 `なのですが、ここを`25 * 25 * 25`にして解像度を上げたことで、モデルの性能が改善したそうです。

- そして特徴マップを全結合層を使って14クラス分類します。特徴マップのセルごとにラベルづけされていて、ほとんどが背景であると予測されるので推論時にはTop-N meanを行います。

### そのほかの工夫

- テストデータにはDICOMデータのタグがなく、CT画像の1ピクセルが何mmか、スライス間隔が何mmかということがわかりませんでした。そこで、各症例の画像をz軸方向に積み重ねたものを入力としたEfficientNetV2-Sモデルを用いて、`x_spacing, y_spacing, z_spacing`を回帰していました。

## 4位解法

![image.png]({{ site.baseurl }}/assets/images/1764485668708-image.png)

- 図に出てくる **青（Preprocessing）→オレンジ（Segmentation Crop）→黄緑（Binary Classification try7）→紫（Pseudo‑Distillation）→水色（Training try8）→ピンク（Inference Ensemble）** の順番に沿って説明します。

### Preprocessing(青)

- `train.csv`を読み込み、各シリーズごとにサイズのそろったスライスのみを採用します。これは、例えばCTの試し撮りなどは画像のサイズが違ったりしするので、そういったものを除くためです。

- 各スライスを`.npy`として保存し、また同時にスライス単位の表`train_sampled.csv`も作成しています。

- modalityをラベルとしてstratified-kfoldを行い、`train_fold.csv`として保存しています。

### Segmentation crop(オレンジ)

- 各患者の DICOM から 3D volume を作り、軸方向に等間隔で 48 枚をサンプリングして .npy（volume）を保存します。

- さらに、元の動脈瘤セグメンテーションマスクから画像内での bbox（x1, x2, y1, y2）を計算（座標を 0〜1 に正規化）して、それらを1行にまとめた `segmentations_window1.csv `を作成。

- ここでViT‑Small‑Plus dino v3 を使って bbox 回帰モデルを学習します。
入力は、segmentations_window1.csv の window_file から読み込んだ volumeで、48枚スライスを 3ch×16 枚画像として reshapeし、`[batch, 16, 3, 128, 128]`の形にしています。出力はx1, x2, y1, y2（0〜1 に正規化された crop 座標）です。損失は L1（MAE）4-fold で学習しています。

- 学習したCropモデルを使い、全train患者/test患者のvolumeに対してbboxを推論しています。

### Binary clasification try7(黄緑)

- このブロックが一番複雑ですが、流れを分けてみます。

3-1. Initial Model Training (try7)
メインノート：AAA_CLS/TRY7_CLS/train1.ipynb
使うデータ：
train_fold.csv（どの患者がどの fold か）
train_sampled.csv（スライス単位画像＋ラベル）
Segmentation Crop で推定した bbox（ROI）
やっていること：

- 各スライス画像を、患者ごとの crop bbox で切り抜き、aneurysm_present + 13 部位 = 14 クラスをターゲットにして、 CoaT/ViT系モデルを学習しています。

- Train v6 → OOF 予測を元に Filter v6（怪しいスライスだけの filt1.csv）。

- Train v8 & v9 → v6 で絞ったデータ＋全体データを使ってより強いモデルを学習しています。また、OOFを使って Filter v8 を作る。
- Train v10 & v13 → v8 Filter などを利用して、さらにチューニングしたモデルです。

- ここで学習したモデル群（特に v8, v9, v10, v13）がtry7 のベストモデルセットになります。

- 同時に、全てのモデルで OOF（Out‑of‑Fold 予測）OUTPUTS/TARGETS/IDS を保存してあります。

- 

### Pseudo‑Distillation(紫)

- 

- 

